\section{Tensores}
\subsection{Convenio de sumación de Einstein}
Antes de comenzar conviene explicar el \textbf{convenio de sumación de Einstein}, que nos va a permitir escribir expresiones de forma compacta que utilizando el símbolo de sumatorio, $\sum$, serían muy largas.

Tomemos por ejemplo un vector \lrg{$\bm{v}$} expresado en función de una base \lrg{$\bm{e}_i$}, con componentes \lrg{$v^i$}, donde \lrg{$i$} no es un exponente, sino un superíndice, esta elección es la clave del criterio.
\lrg{\[\bm{v}=\sum_{i=1}^n{v^i \bm{e}_i}=\sum{v^i \bm{e}_i}\equiv v^i \bm{e}_i\]}
Para poder aplicar el criterio tenemos que tener claro cuales son los posibles valores que puede tomar el índice, en este caso, desde 1 hasta la dimensión del espacio vectorial.

La clave está en que cuando tenemos un superíndice y un súbindice iguales en un mismo término, esto indica una suma con respecto a ese índice, denominado \textbf{índice mudo}, porque solo indica la suma.

Otro ejemplo es aplicado a formas bilineales o endomorfismos, los detalles se detallan posteriormente.
\lrg{\[\phi(\bm{v},\bm{w})=A_{ij} v^i w^j \;\;\;\;\; {f(\bm{v})}^j={M^j}_i \ v^i\]}
Se observa también que en una ecuación los índices no mudos a ambos lados deben coincidir.
\subsection{Covarianza y Contravarianza}
Las componentes de un vector de un espacio vectorial \lrg{$V$} se indican con un superíndice, mientras que los vectores de la base con un subíndice. Por el contrario, las componentes de un vector de \lrg{$V^*$}, el dual, se indican con un subíndice y la base dual se indica con superíndices.
\subsubsection{Base Dual}
\vspace{-25pt}
\lrg{\begin{equation}
\bm{e}^i (\bm{e}_j)= \delta^i_j
\end{equation}}
Recordemos que la \textbf{base dual} se define de esta forma, donde \lrg{$\bm{e}^i$} son los vectores de la base dual, \lrg{$\bm{e}_j$} son los vectores de una base cualquiera de \lrg{$V$}, y \lrg{$\delta^i_j$} es el \textbf{delta de Kronecker}, así pues si los indices son iguales el resultado será 1 y si son distintos será 0.
\subsubsection{Métrica}
De nuevo, recordemos que para medir distancias, recurrimos a un \textbf{producto escalar}, que define una métrica euclidea, de tal forma que
\lrg{\begin{equation}
|\bm{v}|=\sqrt{\phi(\bm{v},\bm{v})}=\sqrt{g_{ij} v^i v^j}
\end{equation}}
De tal forma que \lrg{$g$} es una matriz que define al producto escalar que llamamos la métrica o el \textbf{tensor métrico}, y \lrg{$g_{ij}$} son sus componentes, las cuales dependen de la base de los vectores, de la siguiente manera
\lrg{\begin{equation}
g_{ij} = \bm{e}_i \bm{\cdot} \bm{e}_j
\end{equation}}
Es de interés notar que en una base ortonormal, \lrg{$g_{ij}=\delta_{ij}$}, donde de nuevo, \lrg{$\delta_{ij}$} es el delta de Kronecker.
\subsubsection{Subir y bajar índices}
En el caso \lrg{$\R$}, en el que trabajamos, podemos definir un isomorfismo entre \lrg{$V$} y \lrg{$V^*$} tal que \lrg{$\bm{v}\mapsto \phi(\bm{v},-)$}, dónde \lrg{$\bm{v} \in V$} y \lrg{$\phi(\bm{v},-)=g_{ij} v^i \bm{e^j}$}, donde \lrg{$\bm{e^j}$} es la base dual, así llegamos a
\lrg{\begin{equation}
\begin{matrix}
v_j = g_{ji} v^i && v^j = g^{ji} v_i \\
\bm{e}^j = g^{ji} \bm{e}_i && \bm{e}_j = g_{ji} \bm{e}^i
\end{matrix} \;\;\;\;\; g^{ij} = \left(g_{ij}\right)^{-1} \iff g^{ij} g_{jk} = \delta^i_k
\label{1.4}
\end{equation}}
Esto es importante pues nos permite pasar las coordendas de un vector de \lrg{$V$} a \lrg{$V^*$} y viceversa, o interpretar los elementos de \lrg{$V^*$} como elementos de \lrg{$V$}.
\subsubsection{Covarianza y Contravarianza}
Los superíndices se denominan elementos \textbf{contravariantes}, mientras que los subíndices se denominan elementos \textbf{covariantes}. Esto indica como se transforman estos elementos cuando realizamos un cambio de base.

Si tomamos dos bases de \lrg{$V$}, tenemos que \lrg{$({v'})^j = {S^j}_i \ v^i$}, mientras que para un elemento de \lrg{$V^*$}, si suponemos que esta en una base original y lo pasamos a una base nueva, como lo podemos interpretar como una transformación lineal desde un elemento de \lrg{$V$} hasta \lrg{$\R$}, para que cambiar de base otra, tenemos que multiplicar por la matriz de cambio de base de la base nueva a la antigua, que es la inversa de \lrg{$S$}, pues \lrg{$S$} es la matriz de cambio de base de la antigua a la nueva, así, llegamos a que \lrg{$(v')_j = {(S^{-1})_j}^i \ v_i$}.

Así, los elementos covariantes y contravariantes, al realizar un cambio de base, se transforman de forma inversa.

En el ejemplo que he mostrado se ve como como las componentes de un vector de \lrg{$V$} son contravariantes y las de un vector de \lrg{$V^*$} son covariantes.

Otro ejemplo es por ejemplo las coordenadas de un vector de \lrg{$V$} en una base, que son contravariantes, y los vectores de una base expresados en otra, que son covariantes, puesto que si S es la matriz de cambio de la base vieja a la nueva, su inversa, la matriz de cambio de la base nueva a la vieja contiene como columnas a los vectores de la base nueva expresados en función de la base vieja, así tenemos \lrg{$(\bm{e}')_j = {(S^{-1})^i}_j \ \bm{e}_i$}.

Si interpretamos \ref{1.4} como un cambio de base entonces también podemos observar esta misma naturaleza.
\subsection{Definición de tensor}
\subsubsection{Como forma multilineal}
Un tensor puede definirse como una \textbf{forma multilineal} que manda elementos tanto de \lrg{$V$} como de \lrg{$V^*$} a \lrg{$\R$}, tal que:
\lrg{\[T: V^1 \times \dots \times V^k \rightarrow \R \;\;\;\; \; V^i = \{V\mbox{ o }V^*\}\]}
En muchas ocasiones se colocan los \lrg{$V^*$} todos junto en la izquierda y el resto son \lrg{$V$}, de esta forma se habla de tensores \lrg{$(p,q)$}, donde \lrg{$p$} es el orden contravariante y \lrg{$q$} es el orden covariante.

Es importante notar que no siempre tienen por que ir en este orden ni estar organizadas así las entradas del tensor, pueden estar mezcladas y en cualquier orden, pero este es el más común.

Por ejemplo una forma bilineal, como he indicado antes, sería un tensor \lrg{$(0,2)$} que manda dos vectores de \lrg{$V$} a \lrg{$\R$}, y tiene la forma \lrg{$\phi(\bm{v},\bm{w})=A_{ij} v^i w^j$}, donde \lrg{$A_{ij}$} son las componentes del tensor, que podemos organizar en un \textit{array} de orden 2 (una matriz).

Otro ejemplo es un endomorfismo, que podemos interpretar como un tensor \lrg{$(1,1)$} donde dejamos la entrada de \lrg{$V^*$} abierta o viceversa. Adoptaría la forma de \lrg{${f(-,\bm{w})}^j={M^j}_i \ w^i$}, \lrg{${f(\bm{v},-)}_j={M^i}_j \ v_i$} o \lrg{${f(\bm{v},\bm{w})}={M^i}_j \ v_i w^j$} (este último una forma bilineal mixta).
A las dos primeras expresiones se las denomina \textbf{contracción de un tensor}.

En general las componentes de un tensor \lrg{$(p,q)$} pueden organizarse en un \textit{array} de orden \lrg{$p+q$}. Un tensor de orden total 0 es un escalar, uno de orden total 1 es un elemento de \lrg{$V$} o \lrg{$V^*$} y sus componentes se representan en una lista. Un tensor de orden total 2 puede ser una forma bilineal de \lrg{$V$} o \lrg{$V^*$} o un endomorfismo de \lrg{$V$} o \lrg{$V^*$}, y sus componentes se pueden representar en una matriz. Con \textit{vectores de matrices} o \textit{hipermatrices} para tensores de orden total 3 y así sucesivamente.


\subsubsection{Como elemento de un Espacio Tensorial}

Utilizando una operación llamada \textbf{producto tensorial}, podemos crear multitud de espacios vectoriales denominados espacios tensoriales haciento productos tensoriales de \lrg{$V$} y \lrg{$V^*$}.
\lrg{\[\mathcal{T} = V^1 \otimes \dots \otimes V^k \;\;\;\; \; V^i = \{V\mbox{ o }V^*\}\]}
Al igual que en el caso anterior podemos tener cualquier combinación, pero vamos a centrarnos en la siguiente y generalizaremos
\lrg{\[\mathcal{T}=V \otimes V\]}
En este caso, la base de \lrg{$\mathcal{T}$} serán los elementos abstractos \lrg{$\{\bm{e}_i \otimes \bm{e}_j\}$}, de los cuales en este caso habrá \lrg{$n^2$}. Entonces podremos expresar cualquier elemento \lrg{$\mathbb{T} \in \mathcal{T}$} como una combinación lineal de esa base, \lrg{$\mathbb{T} = T^{ij} \ \bm{e}_i \otimes \bm{e}_j$}

Además definimos un producto escalar entre dos tensores heredado del espacio del que prodece el espacio tensorial, de la siguiente forma:
\lrg{\begin{equation}
(\bm{e}_i \otimes \bm{e}_j) \bm{\cdot} (\bm{e}_k \otimes \bm{e}_l)=(\bm{e}_i \bm{\cdot} \bm{e}_k) (\bm{e}_j \bm{\cdot} \bm{e}_l)=g_{ik} \ g_{jl}
\end{equation}}
Podemos además hacer el producto tensorial de dos vectores cualesquiera de \lrg{$V$} y el resultado será un tensor de \lrg{$\mathcal{T}$} de la siguiente forma:
\lrg{\begin{equation}
\bm{v} \otimes \bm{w} = (v^i \bm{e}_i) \otimes (w^j \bm{e}_j)= v^i w^j \ \bm{e}_i \otimes \bm{e}_j
\end{equation}}
Puesto que el producto tensorial es una operación bilineal.

Para recuperar el significado anterior de forma multilineal lo definimos de la siguiente manera:
\lrg{\begin{equation}\begin{split}
\mathbb{T}(\bm{v},\bm{w}) & = \mathbb{T} \bm{\cdot} (\bm{v} \otimes \bm{w}) = \left[T^{ij} \ \bm{e}_i \otimes \bm{e}_j \right] \bm{\cdot} \left[(v^k \bm{e}_k) \otimes (w^l \bm{e}_l) \right]= \\ & = T^{ij} v^k w^l \ (\bm{e}_i \otimes \bm{e}_j) \bm{\cdot} (\bm{e}_k \otimes \bm{e}_l) = T^{ij} v^k g_{ik} w^l g_{jl} = \\ & = T^{ij} v_i w_j = T_{kl} v^k w^l
\end{split}\end{equation}}
En este caso, por como esta definido el tensor, vemos que al final hay que expresar los vectores como elementos del dual, o el tensor como uno covariante.
\subsubsection{Transformación de tensores}
La propiedad más importante de un tensor es como se transforma durante cambios de base, que es en realidad la característica que los define
\lrg{\begin{equation}\begin{matrix}
\tensor*[]{(\mathbb{T}')}{^{i_1 \dots i_r}_{j_1 \dots j_s}}= T^{i_1}_{k_1} \dots T^{i_r}_{k_r} \ S^{l_1}_{j_1} \dots S^{l_r}_{j_r} \ \tensor*[]{\mathbb{T}}{^{k_1 \dots k_r}_{l_1 \dots l_s}} \\
\tensor*[]{(\mathbb{T})}{^{i_1 \dots i_r}_{j_1 \dots j_s}}= S^{i_1}_{k_1} \dots S^{i_r}_{k_r} \ T^{l_1}_{j_1} \dots T^{l_r}_{j_r} \ \tensor*[]{(\mathbb{T}')}{^{k_1 \dots k_r}_{l_1 \dots l_s}}
\end{matrix}\end{equation}}
Dónde \lrg{$S$} es la matriz de cambio de base de la base nueva a la antigua y \lrg{$T$} al revés y su inversa.
Es decir, las componentes contravariantes se transforman de forma contravariante y lo mismo para las covariantes.